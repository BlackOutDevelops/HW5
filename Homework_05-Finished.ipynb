{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_05_Problem.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19ldCWlJ7REW"
      },
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "This is the 5th assignment for CAP 4630 and we will train an AI-based explorer to play a game by reinforcement learing. As domestrated below, in this game, the treasure (denoted by T) is on the right-most and the explorer (denoted by o) will learn to get the treasure by moving left and right. The explorer will be rewarded when it gets the treasure.  After serveral epoches, the explorer will learn how to get the treasure faster and finally it will go to the treasure by moving to right directly. \\\n",
        "\n",
        "Episode 1, Step1: o----T   \\\n",
        "... \\\n",
        "Episode 1, Step6: ---o-T   \\\n",
        "... \\\n",
        "Episode 1, Step10: -o---T \\\n",
        "... \\\n",
        "Episode 1, Step15: ----oT (finished) \\\n",
        "\n",
        "You will use **\"Tasks\"** and **\"Hints\"** to finish the work. **(Total 100 Points)** \\\n",
        "\n",
        "**Task Overview:**\n",
        "- Train the explorer getting the treasure quickly through Q-learning method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0Cmvbnj7REa"
      },
      "source": [
        "## 1 Achieve Q-learning method ##\n",
        "### 1.1 Model Preparation**(5 Points)**\n",
        "\n",
        "Import useful packages and prepare hyperpaprameters for Q-learning methods. \n",
        "\n",
        "**Tasks:**\n",
        "1. Import numpy and rename it to np.\n",
        "2. Import pandas and rename it to pd.\n",
        "3. Import the library \"time\"\n",
        "4. Set the parameter as suggested\n",
        "\n",
        "**Hints:**\n",
        "1. For your first trial, you may set as it is\n",
        "2. You may explore other possibilities here when you complete the whole homework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4i1-N7o7REb"
      },
      "source": [
        "#import packages here\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "N_STATES = 6   # the width of 1-dim world\n",
        "ACTIONS = ['left', 'right']     # the available actions to use\n",
        "EPSILON = 0.9   # the degree of greedy (0＜ε＜1)\n",
        "ALPHA = 0.1     # learning rate (0＜α≤1)\n",
        "GAMMA = 0.9    # discount factor (0＜γ＜1)\n",
        "MAX_EPOCHES = 13   # the max epoches\n",
        "FRESH_TIME = 0.3    # the interval time"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6x0keZ07REc"
      },
      "source": [
        "### 1.2 Q table**(10 Points)**\n",
        "\n",
        "Q table is a [states * actions] matrix, which stores Q-value of taking one action in that specific state. For example, the following Q table means in state s3, it is more likely to choose a1 because it's Q-value is 5.31 which is higher than Q-value 2.33 for a0 in s3(refer to Lecture slides 16, page 35).\n",
        "![](https://drive.google.com/uc?export=view&id=1WGh7NYyYw6ccrxbDVdfbJmb_IhBfUyFf)\n",
        "\n",
        "**Tasks:**\n",
        "1. define the build_q_table function\n",
        "2. **Print Out** defined Q-table\n",
        "\n",
        "**Hints:**\n",
        "1. Using pd.DataFrame to define the Q-table.(https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)\n",
        "2. Initialize the Q-table with all zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3VlakdE7REc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc1714e6-a085-4f71-b782-b86c71181c8e"
      },
      "source": [
        "#define the function here\n",
        "def build_q_table(n_states, actions): #add your code here\n",
        "  elements = np.zeros((len(actions), n_states))\n",
        "  columns = np.empty(n_states, dtype=int)\n",
        "  for s in range(0, n_states):\n",
        "    columns[s] = s\n",
        "\n",
        "  df = pd.DataFrame(elements, columns=columns, index=actions)\n",
        "  return df\n",
        "\n",
        "q_table = build_q_table(N_STATES, ACTIONS)\n",
        "print(q_table, \"\\n\")"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0    1    2    3    4    5\n",
            "left   0.0  0.0  0.0  0.0  0.0  0.0\n",
            "right  0.0  0.0  0.0  0.0  0.0  0.0 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvGAtNQm7REe"
      },
      "source": [
        "### 1.3 Define action**(15 Points)**\n",
        "\n",
        "In this section, we are going to define how an actor picks the actions. We introduce ε-greedy (In lecture slide 16, page 35). In the initial exploring stage, the explorer knows little about the environment. Therefore, it is better to explore randomly instead of greedy. ε-greedy is the value to control the degree of greedy. It can be changed with time lapsing. In this homework, we set it as fixed value EPSILON = 0.9. You can change it to explore the final effect.\n",
        "\n",
        "**Tasks:**\n",
        "1. define the choose_action function\n",
        "2. **Print Out** sample action.\n",
        "\n",
        "**Hints:**\n",
        "1. You need to define two patterns: 1) non-greedy (i.e., random); 2) greedy.\n",
        "2. Non-greedy should occupy (1-ε) senario while greedy should occupy ε senario. In this case, it means Non-greedy occupys 10% senario while greedy occupys 90% senario. (you could implement it by comparing a random number ranging from 0 to 1 with ε)\n",
        "3. In the non-greedy pattern, the actor should choose the actions randomly.\n",
        "4. In the greedy pattern, the actor should choose the higher Q-value action.\n",
        "5. Don't forget the initial state which means all Q-value are zero and actor cannot choose greedily. You can treat it as non-greedy pattern."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qps8xVq87REe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc26c523-6cac-469e-c52f-8303cc20fc14"
      },
      "source": [
        "import random\n",
        "# define the function here\n",
        "# Given state and Q-table, choose action\n",
        "def choose_action(state, q_table):\n",
        "      # pick all actions from this state\n",
        "    random_num = random.randint(0,1)\n",
        "    turn = ['left', 'right']\n",
        "    if (random_num >= EPSILON)  or (q_table.loc['left', state] == 0 and q_table.loc['right', state] == 0):  # non-greedy or non-explored\n",
        "        action_name = random.choice(turn)\n",
        "    else:                                                                                              # greedy\n",
        "        if q_table.loc['left', state] < q_table.loc['right', state]:\n",
        "          action_name = 'right'\n",
        "        else:\n",
        "          action_name = 'left'\n",
        "    return action_name\n",
        "\n",
        "sample_action = choose_action(0, q_table)\n",
        "print(sample_action)"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "right\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y0sb9Wr84rV"
      },
      "source": [
        "### 1.4 Interact with the environment**(30 Points)**\n",
        "\n",
        "In this section, we need to give a feedback for our previous action, which means getting reward (R) for next state (S_next) based on current state (S_current) and action (A). In this problem, we get reward R=1 if we move to the treasure T spot, otherwise, we get R=0.\n",
        "\n",
        "**Tasks:**\n",
        "1. define get_env_feedback function\n",
        "**Hints:**\n",
        "1. This function contains two parameters S_current and A(ction), and return S_next and R(eward).\n",
        "2. You need to consider two different senarios: 1) A = right; 2) A = left.\n",
        "3. In the above two senarios, you need to consider the boundary, next state and rewards.\n",
        "4. The update_env function is given to show changes for different steps in different episodes.\n",
        "5. The validation for S_current and Action is shown below.\n",
        "\n",
        "- S_current=0, sample_action = 'right', sample_feedback=(1,0)\n",
        "- S_current=3, sample_action = 'right', sample_feedback=(4,0)\n",
        "- S_current=4, sample_action = 'right', sample_feedback=('terminal', 1)\n",
        "- S_current=0, sample_action = 'left', sample_feedback=(0,0)\n",
        "- S_current=3, sample_action = 'left', sample_feedback=(2,0)\n",
        "- S_current=4, sample_action = 'left', sample_feedback=(3, 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xb5Xi8U7REf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194406a0-c5b4-4550-bfe4-59528335200c"
      },
      "source": [
        "#define the function here\n",
        "def get_env_feedback(S_current, A):\n",
        "    # This is how agent will interact with the environment\n",
        "    if A == 'right':    # move right\n",
        "        S_next = S_current + 1\n",
        "    else:   # move left\n",
        "        if S_current == 0:\n",
        "          S_next = 0\n",
        "        else:\n",
        "          S_next = S_current - 1\n",
        "    if S_next == N_STATES - 1:\n",
        "      S_next = 'terminal'\n",
        "      R = 1\n",
        "    else:\n",
        "      R = 0\n",
        "    return S_next, R\n",
        "\n",
        "sample_action = 'left'\n",
        "S_current = 0\n",
        "sample_feedback = get_env_feedback(S_current, sample_action)\n",
        "print(sample_feedback)"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5X__swLIJd3"
      },
      "source": [
        "def update_env(S, episode, step_counter):\n",
        "    # This is how environment be updated\n",
        "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
        "    if S == 'terminal':\n",
        "        interaction = '  Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
        "        print('{}\\n'.format(interaction), end='')\n",
        "        time.sleep(2)\n",
        "    else:\n",
        "        env_list[S] = 'o'\n",
        "        interaction = ''.join(env_list)\n",
        "        print('\\r{}'.format(interaction), end='')\n",
        "        time.sleep(FRESH_TIME)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSESWvv5ANHl"
      },
      "source": [
        "### 1.5 Start Q-learning with defined functions**(40 Points)**\n",
        "\n",
        "In this section, we are going to utilize all the functions defined above to do q-learning based on the optimal policy.\n",
        "![](https://drive.google.com/uc?export=view&id=10ra6mLlBHlhGNTYWwdGANoa6lC1K_7at)\n",
        "\n",
        "**Tasks**:\n",
        "1. define reinforce_learning function\n",
        "\n",
        "**Hints**:\n",
        "1. You should write this function with loops to keep updating q-table until you get to the reward spot.\n",
        "2. We have two loops, one is for different episodes and another one is for steps\n",
        "3. Whenever we take a step to the reward spot, we should end the loop and start another episode.\n",
        "4. Here is one possible example.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1oo-gk710XVXbbeI7AI0uZInrnKtqGqn7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t3feBszAZUx"
      },
      "source": [
        "#define the function here\n",
        "def reinforce_learning():\n",
        "    # main part of RL loop\n",
        "    # build Q-table here\n",
        "    q_table = build_q_table(N_STATES, ACTIONS)\n",
        "    #start training loop\n",
        "    for episode in range(MAX_EPOCHES):\n",
        "        step_counter = 0  #counter for counting steps to reach the treasure\n",
        "        S_current = 0     #start from S_current\n",
        "        is_terminated = 0   #flag to continue or stop the loop\n",
        "        update_env(S_current, episode, step_counter)   #update environment\n",
        "        while not is_terminated:\n",
        "            action = choose_action(S_current, q_table) #choose one action\n",
        "            S_next = get_env_feedback(S_current, action)[0] # take action & get next state and reward\n",
        "            #update Q-table\n",
        "            if S_next != 'terminal':                   #if the explorer doesn't get to the treasure\n",
        "                q_target = 0 + GAMMA * np.max(q_table.loc[:, S_next])   # next state is not terminal\n",
        "            else:\n",
        "                q_target = 1     # next state is terminal\n",
        "                is_terminated = 1    # terminate this episode\n",
        "\n",
        "            q_table.loc[action, S_current] = q_table.loc[action, S_current] + ALPHA*(q_target - q_table.loc[action, S_current])  # update Q-table\n",
        "            S_current = S_next  # move to next state\n",
        "\n",
        "            update_env(S_current, episode, step_counter+1)\n",
        "            step_counter += 1\n",
        "    return q_table\n",
        "    "
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSRgnyFdAdfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a1ead0-5aaf-46d5-9913-65b2f927e618"
      },
      "source": [
        "#main function to run \n",
        "if __name__ == \"__main__\":\n",
        "    q_table = reinforce_learning()\n",
        "    print('\\r\\nQ-table:\\n')\n",
        "    print(q_table)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----oT  Episode 1: total_steps = 54\n",
            "----oT  Episode 2: total_steps = 25\n",
            "----oT  Episode 3: total_steps = 8\n",
            "----oT  Episode 4: total_steps = 9\n",
            "----oT  Episode 5: total_steps = 7\n",
            "----oT  Episode 6: total_steps = 7\n",
            "----oT  Episode 7: total_steps = 7\n",
            "----oT  Episode 8: total_steps = 13\n",
            "----oT  Episode 9: total_steps = 5\n",
            "----oT  Episode 10: total_steps = 13\n",
            "----oT  Episode 11: total_steps = 9\n",
            "----oT  Episode 12: total_steps = 16\n",
            "----oT  Episode 13: total_steps = 11\n",
            "\n",
            "Q-table:\n",
            "\n",
            "              0         1         2         3         4    5\n",
            "left   0.005867  0.003457  0.008216  0.036586  0.091107  0.0\n",
            "right  0.020883  0.065372  0.197684  0.424265  0.745813  0.0\n"
          ]
        }
      ]
    }
  ]
}